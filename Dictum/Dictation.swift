//
//  Dictation.swift
//  Dictum
//
//  ASR: VolumeManager, AccessibilityHelper, Parakeet, Deepgram
//

import SwiftUI
@preconcurrency import AVFoundation
import FluidAudio

// MARK: - Sendable Bool Box (for closure capture)
private final class BoolBox: @unchecked Sendable {
    var value: Bool
    init(_ value: Bool) { self.value = value }
}

// MARK: - Volume Manager
class VolumeManager: @unchecked Sendable {
    static let shared = VolumeManager()
    private var savedVolume: Int?

    func getCurrentVolume() -> Int? {
        let process = Process()
        let pipe = Pipe()

        defer {
            try? pipe.fileHandleForReading.close()
            if process.isRunning { process.terminate() }
        }

        process.executableURL = URL(fileURLWithPath: "/usr/bin/osascript")
        process.arguments = ["-e", "output volume of (get volume settings)"]
        process.standardOutput = pipe

        do {
            try process.run()
            process.waitUntilExit()

            let data = pipe.fileHandleForReading.readDataToEndOfFile()
            if let output = String(data: data, encoding: .utf8)?.trimmingCharacters(in: .whitespacesAndNewlines),
               let volume = Int(output) {
                return volume
            }
        } catch {
            NSLog("‚ùå Failed to get volume: \(error)")
        }
        return nil
    }

    func setVolume(_ level: Int) {
        let clampedLevel = max(0, min(100, level))
        let process = Process()

        defer {
            if process.isRunning { process.terminate() }
        }

        process.executableURL = URL(fileURLWithPath: "/usr/bin/osascript")
        process.arguments = ["-e", "set volume output volume \(clampedLevel)"]

        do {
            try process.run()
            process.waitUntilExit()
            NSLog("üîä Volume set to \(clampedLevel)")
        } catch {
            NSLog("‚ùå Failed to set volume: \(error)")
        }
    }

    func saveAndReduceVolume(targetVolume: Int = 15) {
        savedVolume = getCurrentVolume()
        if let current = savedVolume {
            NSLog("üíæ Saved volume: \(current)")
            if current > targetVolume {
                setVolume(targetVolume)
            }
        }
    }

    func restoreVolume() {
        if let saved = savedVolume {
            setVolume(saved)
            NSLog("üîä Restored volume to: \(saved)")
            savedVolume = nil
        }
    }
}

// MARK: - Permission Manager
/// –¶–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è–º–∏ macOS
/// –ü—Ä–∏–Ω—Ü–∏–ø—ã:
/// 1. –ù–µ –¥—É–±–ª–∏—Ä–æ–≤–∞—Ç—å —Å–∏—Å—Ç–µ–º–Ω—ã–µ –¥–∏–∞–ª–æ–≥–∏ ‚Äî –µ—Å–ª–∏ API –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –¥–∏–∞–ª–æ–≥, –Ω–µ –æ—Ç–∫—Ä—ã–≤–∞–µ–º Settings
/// 2. –û—Ç–∫—Ä—ã–≤–∞—Ç—å Settings —Ç–æ–ª—å–∫–æ –∫–æ–≥–¥–∞ –Ω—É–∂–Ω–æ (—É–∂–µ –æ—Ç–∫–∞–∑–∞–Ω–æ –∏–ª–∏ –Ω–µ—Ç –¥–∏–∞–ª–æ–≥–∞)
/// 3. Screen Recording ‚Äî —Ç—Ä–∏–≥–≥–µ—Ä–∏–º —Ä–µ–∞–ª—å–Ω—ã–π capture —á—Ç–æ–±—ã –ø–æ—è–≤–∏—Ç—å—Å—è –≤ —Å–ø–∏—Å–∫–µ
class PermissionManager: @unchecked Sendable {
    static let shared = PermissionManager()

    // MARK: - Check Permissions

    /// –ü—Ä–æ–≤–µ—Ä–∫–∞ Accessibility (–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –¥–æ—Å—Ç—É–ø)
    func hasAccessibility() -> Bool {
        AXIsProcessTrusted()
    }

    /// –ü—Ä–æ–≤–µ—Ä–∫–∞ Microphone
    func hasMicrophone() -> Bool {
        AVCaptureDevice.authorizationStatus(for: .audio) == .authorized
    }

    /// –ü—Ä–æ–≤–µ—Ä–∫–∞ Screen Recording
    func hasScreenRecording() -> Bool {
        CGPreflightScreenCaptureAccess()
    }

    // MARK: - Request Permissions

    /// Accessibility: –°–∏—Å—Ç–µ–º–Ω—ã–π –¥–∏–∞–ª–æ–≥ —Å–∞–º –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç Settings –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
    /// –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç true –µ—Å–ª–∏ —É–∂–µ –µ—Å—Ç—å —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ
    @discardableResult
    func requestAccessibility() -> Bool {
        // –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç—Ä–æ–∫–æ–≤—ã–π –∫–ª—é—á –Ω–∞–ø—Ä—è–º—É—é –¥–ª—è Swift 6 concurrency safety
        let options: NSDictionary = ["AXTrustedCheckOptionPrompt": true]
        let result = AXIsProcessTrustedWithOptions(options)
        NSLog("üîê Accessibility request: \(result ? "granted" : "will show dialog")")
        return result
    }

    /// Microphone: –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–π –¥–∏–∞–ª–æ–≥ –µ—Å–ª–∏ –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ
    /// –û—Ç–∫—Ä—ã–≤–∞–µ–º Settings —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —É–∂–µ –æ—Ç–∫–∞–∑–∞–Ω–æ
    func requestMicrophone(completion: @escaping @Sendable (Bool) -> Void) {
        let status = AVCaptureDevice.authorizationStatus(for: .audio)
        NSLog("üé§ Microphone status: \(status.rawValue)")

        switch status {
        case .notDetermined:
            // –ü–æ–∫–∞–∑–∞—Ç—å —Å–∏—Å—Ç–µ–º–Ω—ã–π –¥–∏–∞–ª–æ–≥ (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –¥–æ–±–∞–≤–∏—Ç –≤ —Å–ø–∏—Å–æ–∫)
            AVCaptureDevice.requestAccess(for: .audio) { granted in
                NSLog("üé§ Microphone dialog result: \(granted)")
                DispatchQueue.main.async {
                    completion(granted)
                }
            }
        case .denied, .restricted:
            // –£–∂–µ –æ—Ç–∫–∞–∑–∞–Ω–æ ‚Äî –æ—Ç–∫—Ä—ã–≤–∞–µ–º Settings
            NSLog("üé§ Microphone already denied, opening settings")
            openPrivacySettings(section: "Microphone")
            completion(false)
        case .authorized:
            completion(true)
        @unknown default:
            completion(false)
        }
    }

    /// Screen Recording: –¢—Ä–∏–≥–≥–µ—Ä–∏–º —Ä–µ–∞–ª—å–Ω—ã–π capture —á—Ç–æ–±—ã:
    /// 1. –ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –ø–æ—è–≤–∏–ª–æ—Å—å –≤ —Å–ø–∏—Å–∫–µ
    /// 2. –ü–æ–∫–∞–∑–∞–ª—Å—è —Å–∏—Å—Ç–µ–º–Ω—ã–π –¥–∏–∞–ª–æ–≥ (–µ—Å–ª–∏ –ø–µ—Ä–≤—ã–π —Ä–∞–∑)
    func requestScreenRecording() {
        NSLog("üìπ Requesting Screen Recording permission...")

        // CGWindowListCreateImage —Ç—Ä–∏–≥–≥–µ—Ä–∏—Ç —Å–∏—Å—Ç–µ–º–Ω—ã–π –¥–∏–∞–ª–æ–≥ –∏ –¥–æ–±–∞–≤–ª—è–µ—Ç –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –≤ —Å–ø–∏—Å–æ–∫
        // Deprecated –≤ macOS 14, –Ω–æ –Ω—É–∂–µ–Ω –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ –∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –≤ TCC
        if #available(macOS 14.0, *) {
            // –ù–∞ macOS 14+ –∏—Å–ø–æ–ª—å–∑—É–µ–º CGRequestScreenCaptureAccess
            // –≠—Ç–æ –ø–æ–∫–∞–∂–µ—Ç –¥–∏–∞–ª–æ–≥ –∏ –¥–æ–±–∞–≤–∏—Ç –≤ —Å–ø–∏—Å–æ–∫
            CGRequestScreenCaptureAccess()
        } else {
            // Legacy fallback
            let _ = CGWindowListCreateImage(
                CGRect(x: 0, y: 0, width: 1, height: 1),
                .optionOnScreenOnly,
                kCGNullWindowID,
                .bestResolution
            )
        }

        // –û—Ç–∫—Ä—ã–≤–∞–µ–º Settings —á—Ç–æ–±—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –º–æ–≥ –≤–∫–ª—é—á–∏—Ç—å —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ
        openPrivacySettings(section: "ScreenCapture")
    }

    // MARK: - Open System Settings

    /// –û—Ç–∫—Ä—ã—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é —Å–µ–∫—Ü–∏—é Privacy & Security
    /// section: "Accessibility", "Microphone", "ScreenCapture", etc.
    func openPrivacySettings(section: String? = nil) {
        // –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≤–µ—Ä—Å–∏—é macOS
        let osVersion = ProcessInfo.processInfo.operatingSystemVersion
        let isTahoeOrLater = osVersion.majorVersion >= 26

        var urlString = "x-apple.systempreferences:com.apple.preference.security"

        // –ù–∞ macOS 26+ (Tahoe) –ø–∞—Ä–∞–º–µ—Ç—Ä—ã Privacy_* –º–æ–≥—É—Ç –≤—ã–∑—ã–≤–∞—Ç—å –∫—Ä–∞—à
        // –ü—Ä–æ–±—É–µ–º —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–º —Ç–æ–ª—å–∫–æ –Ω–∞ –±–æ–ª–µ–µ —Ä–∞–Ω–Ω–∏—Ö –≤–µ—Ä—Å–∏—è—Ö
        if let section = section, !isTahoeOrLater {
            urlString += "?Privacy_\(section)"
        }

        NSLog("üîß Opening System Settings: \(urlString) (macOS \(osVersion.majorVersion))")

        if let url = URL(string: urlString) {
            NSWorkspace.shared.open(url)
        }
    }
}

// MARK: - Legacy Compatibility (–¥–ª—è —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –∫–æ–¥–∞)
typealias AccessibilityHelper = PermissionManager

extension PermissionManager {
    /// Legacy: –ø—Ä–æ–≤–µ—Ä–∫–∞ accessibility
    static func checkAccessibility() -> Bool {
        shared.hasAccessibility()
    }

    /// Legacy: –∑–∞–ø—Ä–æ—Å accessibility
    static func requestAccessibility() {
        shared.requestAccessibility()
    }

    /// Legacy: –ø—Ä–æ–≤–µ—Ä–∫–∞ screen recording
    static func hasScreenRecordingPermission() -> Bool {
        shared.hasScreenRecording()
    }

    /// Legacy: –∑–∞–ø—Ä–æ—Å screen recording
    static func requestScreenRecordingPermission() {
        shared.requestScreenRecording()
    }
}

// MARK: - Parakeet Model Status
enum ParakeetModelStatus: Equatable {
    case notChecked          // –ï—â—ë –Ω–µ –ø—Ä–æ–≤–µ—Ä—è–ª–∏
    case checking            // –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –º–æ–¥–µ–ª–∏
    case notDownloaded       // –ú–æ–¥–µ–ª—å –Ω–µ —Å–∫–∞—á–∞–Ω–∞
    case downloading         // –ò–¥—ë—Ç —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ (~600 MB)
    case loading             // –ó–∞–≥—Ä—É–∑–∫–∞ –≤ –ø–∞–º—è—Ç—å (–∫–æ–º–ø–∏–ª—è—Ü–∏—è CoreML)
    case ready               // –ì–æ—Ç–æ–≤–∞ –∫ —Ä–∞–±–æ—Ç–µ
    case error(String)       // –û—à–∏–±–∫–∞

    var displayText: String {
        switch self {
        case .notChecked: return "–ü—Ä–æ–≤–µ—Ä–∫–∞..."
        case .checking: return "–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –º–æ–¥–µ–ª–∏..."
        case .notDownloaded: return "–ú–æ–¥–µ–ª—å –Ω–µ —Å–∫–∞—á–∞–Ω–∞"
        case .downloading: return "–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏..."
        case .loading: return "–ó–∞–≥—Ä—É–∑–∫–∞ –≤ –ø–∞–º—è—Ç—å..."
        case .ready: return "Parakeet v3 –≥–æ—Ç–æ–≤–∞"
        case .error(let msg): return "–û—à–∏–±–∫–∞: \(msg)"
        }
    }
}

// MARK: - Parakeet ASR Provider (Local)
class ParakeetASRProvider: ObservableObject, @unchecked Sendable {
    @Published var isRecording = false
    @Published var transcriptionResult: String?
    @Published var interimText: String = ""
    @Published var errorMessage: String?
    @Published var audioLevel: Float = 0.0
    @Published var isModelLoaded = false
    @Published var modelStatus: ParakeetModelStatus = .notChecked
    @Published var downloadedFilesCount: Int = 0
    @Published var totalFilesCount: Int = 0

    private var audioEngine: AVAudioEngine?
    private var asrManager: AsrManager?
    private var models: AsrModels?

    // –ù–∞–∫–æ–ø–ª–µ–Ω–∏–µ –∞—É–¥–∏–æ —Å—ç–º–ø–ª–æ–≤ (batch processing)
    private var audioSamples: [Float] = []
    private let samplesLock = NSLock()

    // –ö–µ—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π AVAudioConverter –¥–ª—è 16 kHz
    private var audioConverter: AVAudioConverter?
    private var outputFormat: AVAudioFormat?
    private var resampledBuffer: AVAudioPCMBuffer?

    // Guard –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è double stop
    private var isStopInProgress = false

    init() {
        Task {
            await checkModelStatus()
            if modelStatus == .notDownloaded {
                return
            }
            await initializeModelsIfNeeded()
        }
    }

    /// –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –º–æ–¥–µ–ª–∏ –±–µ–∑ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
    func checkModelStatus() async {
        await MainActor.run {
            modelStatus = .checking
        }

        let cacheDir = AsrModels.defaultCacheDirectory(for: .v3)
        let exists = AsrModels.modelsExist(at: cacheDir, version: .v3)

        await MainActor.run {
            if exists {
                modelStatus = .loading
            } else {
                modelStatus = .notDownloaded
            }
        }
    }

    deinit {
        audioEngine?.inputNode.removeTap(onBus: 0)
        audioEngine?.stop()
        audioEngine = nil
        audioConverter = nil
        resampledBuffer = nil
        NSLog("üóëÔ∏è ParakeetASRProvider –æ—Å–≤–æ–±–æ–∂–¥—ë–Ω")
    }

    /// –£–¥–∞–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏–∑ –∫—ç—à–∞
    func deleteModel() async {
        if isRecording {
            await stopRecordingAndTranscribe()
        }

        asrManager = nil
        models = nil

        await MainActor.run {
            isModelLoaded = false
            modelStatus = .notChecked
        }

        let cacheDir = AsrModels.defaultCacheDirectory(for: .v3)

        do {
            if FileManager.default.fileExists(atPath: cacheDir.path) {
                try FileManager.default.removeItem(at: cacheDir)
                NSLog("üóëÔ∏è –£–¥–∞–ª–µ–Ω–∞ –º–æ–¥–µ–ª—å Parakeet v3: \(cacheDir.path)")
            }

            await MainActor.run {
                modelStatus = .notDownloaded
            }
        } catch {
            NSLog("‚ùå –û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è –º–æ–¥–µ–ª–∏: \(error.localizedDescription)")
            await MainActor.run {
                modelStatus = .error("–ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å: \(error.localizedDescription)")
            }
        }
    }

    /// –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π (—Å–∫–∞—á–∏–≤–∞–µ—Ç ~600 MB –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—É—Å–∫–µ)
    func initializeModelsIfNeeded() async {
        guard !isModelLoaded else { return }

        do {
            let cacheDir = AsrModels.defaultCacheDirectory(for: .v3)
            let modelExists = AsrModels.modelsExist(at: cacheDir, version: .v3)

            if modelExists {
                await MainActor.run { modelStatus = .loading }
                NSLog("üß† –ó–∞–≥—Ä—É–∑–∫–∞ Parakeet v3 –∏–∑ –∫—ç—à–∞...")
            } else {
                await MainActor.run { modelStatus = .downloading }
                NSLog("‚¨áÔ∏è –°–∫–∞—á–∏–≤–∞–Ω–∏–µ Parakeet v3 (~600 MB)...")
            }

            let downloadedModels = try await AsrModels.downloadAndLoad(version: .v3)

            await MainActor.run { modelStatus = .loading }
            NSLog("üß† –ö–æ–º–ø–∏–ª—è—Ü–∏—è CoreML –º–æ–¥–µ–ª–µ–π...")

            let manager = AsrManager(config: .default)
            try await manager.initialize(models: downloadedModels)

            self.models = downloadedModels
            self.asrManager = manager

            await MainActor.run {
                isModelLoaded = true
                modelStatus = .ready
            }

            NSLog("‚úÖ Parakeet v3 –º–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞ (25 —è–∑—ã–∫–æ–≤, ~190x real-time)")
        } catch {
            NSLog("‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: \(error.localizedDescription)")
            await MainActor.run {
                modelStatus = .error(error.localizedDescription)
                errorMessage = "–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: \(error.localizedDescription)"
            }
        }
    }

    func startRecording() async {
        guard !isRecording else {
            NSLog("‚ö†Ô∏è –õ–æ–∫–∞–ª—å–Ω–∞—è –∑–∞–ø–∏—Å—å —É–∂–µ –∏–¥—ë—Ç")
            return
        }

        guard isModelLoaded, asrManager != nil else {
            await MainActor.run {
                if modelStatus == .loading {
                    errorMessage = "–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è..."
                } else {
                    errorMessage = "–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞. –ü–æ–¥–æ–∂–¥–∏—Ç–µ –∏–ª–∏ –æ—Ç–∫—Ä–æ–π—Ç–µ –ù–∞—Å—Ç—Ä–æ–π–∫–∏."
                }
            }
            return
        }

        let hasPermission = await requestMicrophonePermission()
        guard hasPermission else {
            await MainActor.run {
                errorMessage = "–ù–µ—Ç –¥–æ—Å—Ç—É–ø–∞ –∫ –º–∏–∫—Ä–æ—Ñ–æ–Ω—É"
            }
            return
        }

        samplesLock.withLock {
            audioSamples.removeAll()
        }

        await MainActor.run {
            interimText = ""
            transcriptionResult = nil
            isRecording = true
            audioLevel = 0.0
        }

        VolumeManager.shared.saveAndReduceVolume(targetVolume: SettingsManager.shared.volumeLevel)

        let engine = AVAudioEngine()
        let inputNode = engine.inputNode
        let inputFormat = inputNode.outputFormat(forBus: 0)

        guard inputFormat.sampleRate > 0, inputFormat.channelCount > 0 else {
            await MainActor.run {
                errorMessage = "–ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –≤—Ö–æ–¥–Ω–æ–≥–æ –∞—É–¥–∏–æ"
                isRecording = false
            }
            return
        }

        guard let outFmt = AVAudioFormat(commonFormat: .pcmFormatFloat32, sampleRate: 16000, channels: 1, interleaved: false) else {
            await MainActor.run {
                errorMessage = "–û—à–∏–±–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∞ –∞—É–¥–∏–æ"
                isRecording = false
            }
            return
        }
        self.outputFormat = outFmt

        guard let converter = AVAudioConverter(from: inputFormat, to: outFmt) else {
            await MainActor.run {
                errorMessage = "–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –∞—É–¥–∏–æ-–∫–æ–Ω–≤–µ—Ä—Ç–µ—Ä–∞"
                isRecording = false
            }
            return
        }
        self.audioConverter = converter

        let maxOutputFrames = AVAudioFrameCount(outFmt.sampleRate * 0.2)
        self.resampledBuffer = AVAudioPCMBuffer(pcmFormat: outFmt, frameCapacity: maxOutputFrames)

        engine.prepare()

        let bufferSizeForInput = AVAudioFrameCount(inputFormat.sampleRate * 0.1)
        inputNode.installTap(onBus: 0, bufferSize: bufferSizeForInput, format: inputFormat) { [weak self] buffer, _ in
            self?.processAudioBuffer(buffer)
        }

        do {
            try engine.start()
            self.audioEngine = engine
            NSLog("üé§ –õ–æ–∫–∞–ª—å–Ω—ã–π ASR –∑–∞–ø—É—â–µ–Ω (Parakeet v3)")

            await MainActor.run {
                interimText = "–°–ª—É—à–∞—é..."
            }

        } catch {
            inputNode.removeTap(onBus: 0)
            self.audioConverter = nil
            self.outputFormat = nil

            await MainActor.run {
                errorMessage = "–û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞: \(error.localizedDescription)"
                isRecording = false
            }
        }
    }

    private func processAudioBuffer(_ buffer: AVAudioPCMBuffer) {
        // 1. –†–∞—Å—Å—á–∏—Ç–∞—Ç—å —É—Ä–æ–≤–µ–Ω—å –≥—Ä–æ–º–∫–æ—Å—Ç–∏ (RMS)
        if let channelData = buffer.floatChannelData {
            let frameLength = Int(buffer.frameLength)
            var sum: Float = 0.0
            for i in 0..<frameLength {
                let sample = channelData[0][i]
                sum += sample * sample
            }
            let rms = sqrt(sum / Float(max(1, frameLength)))
            let normalizedRms = rms * 50.0
            let level = min(1.0, log10(1 + normalizedRms * 9))

            DispatchQueue.main.async { [weak self] in
                self?.audioLevel = level
            }
        }

        // 2. –†–µ—Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ 16 kHz
        guard let converter = audioConverter,
              let outFmt = outputFormat,
              let outputBuffer = resampledBuffer else {
            return
        }

        guard buffer.format.sampleRate > 0 else { return }

        let ratio = outFmt.sampleRate / Double(buffer.format.sampleRate)
        let outputFrameCount = AVAudioFrameCount(Double(buffer.frameLength) * ratio)

        guard outputFrameCount <= outputBuffer.frameCapacity else {
            NSLog("‚ö†Ô∏è Buffer overflow: need \(outputFrameCount), have \(outputBuffer.frameCapacity)")
            return
        }
        outputBuffer.frameLength = outputFrameCount

        var conversionError: NSError?
        let hasDataBox = BoolBox(true)

        converter.convert(to: outputBuffer, error: &conversionError) { _, outStatus in
            if hasDataBox.value {
                outStatus.pointee = .haveData
                hasDataBox.value = false
                return buffer
            }
            outStatus.pointee = .noDataNow
            return nil
        }

        if let error = conversionError {
            NSLog("‚ùå Audio conversion error: \(error.localizedDescription)")
            return
        }

        // 3. –ù–∞–∫–æ–ø–ª–µ–Ω–∏–µ —Å—ç–º–ø–ª–æ–≤
        if let channelData = outputBuffer.floatChannelData {
            let samples = Array(UnsafeBufferPointer(start: channelData[0], count: Int(outputBuffer.frameLength)))
            samplesLock.withLock {
                audioSamples.append(contentsOf: samples)
            }
        }
    }

    func stopRecordingAndTranscribe() async {
        guard !isStopInProgress else {
            NSLog("‚ö†Ô∏è stopRecording already in progress, skipping")
            return
        }
        isStopInProgress = true
        defer { isStopInProgress = false }

        audioEngine?.inputNode.removeTap(onBus: 0)
        audioEngine?.stop()
        audioEngine = nil

        audioConverter?.reset()
        audioConverter = nil
        outputFormat = nil
        resampledBuffer = nil

        await MainActor.run {
            interimText = "–û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é..."
        }

        let samplesToProcess = samplesLock.withLock { audioSamples }

        guard let asrManager = asrManager, !samplesToProcess.isEmpty else {
            await MainActor.run {
                isRecording = false
                interimText = ""
            }
            VolumeManager.shared.restoreVolume()
            return
        }

        do {
            NSLog("üîÑ –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è \(samplesToProcess.count) —Å—ç–º–ø–ª–æ–≤ (~\(String(format: "%.1f", Double(samplesToProcess.count) / 16000))s –∞—É–¥–∏–æ)...")

            let result = try await asrManager.transcribe(samplesToProcess)
            let text = result.text.trimmingCharacters(in: .whitespaces)

            await MainActor.run {
                transcriptionResult = text.isEmpty ? nil : text
                isRecording = false
                interimText = ""
            }

            NSLog("‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç (Parakeet): \(text)")

        } catch {
            let errorDesc = error.localizedDescription
            NSLog("‚ùå –û—à–∏–±–∫–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏: \(errorDesc)")

            if !errorDesc.contains("Must be at least 1 second") {
                await MainActor.run {
                    errorMessage = "–û—à–∏–±–∫–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏: \(errorDesc)"
                    isRecording = false
                    interimText = ""
                }
            } else {
                NSLog("‚ÑπÔ∏è –ó–∞–ø–∏—Å—å —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∞—è, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                await MainActor.run {
                    isRecording = false
                    interimText = ""
                }
            }
        }

        samplesLock.withLock {
            audioSamples.removeAll()
        }

        VolumeManager.shared.restoreVolume()
    }

    private func requestMicrophonePermission() async -> Bool {
        await withCheckedContinuation { continuation in
            AVCaptureDevice.requestAccess(for: .audio) { granted in
                continuation.resume(returning: granted)
            }
        }
    }
}

// MARK: - Alias for backward compatibility
typealias SherpaASRProvider = ParakeetASRProvider

// MARK: - Real-time Streaming Audio Manager (WebSocket)
class AudioRecordingManager: NSObject, ObservableObject, URLSessionWebSocketDelegate, @unchecked Sendable {
    @Published var isRecording = false
    @Published var isTranscribing = false
    @Published var errorMessage: String?
    @Published var transcriptionResult: String?
    @Published var interimText: String = ""
    @Published var appendMode: Bool = false
    @Published var audioLevel: Float = 0.0

    private var audioEngine: AVAudioEngine?
    private var webSocket: URLSessionWebSocketTask?
    private var urlSession: URLSession!
    private var finalTranscript: String = ""
    private var audioBuffer: [Data] = []

    private var _webSocketConnected: Bool = false
    private let webSocketConnectedLock = NSLock()
    private var webSocketConnected: Bool {
        get { webSocketConnectedLock.withLock { _webSocketConnected } }
        set { webSocketConnectedLock.withLock { _webSocketConnected = newValue } }
    }

    private var isClosingWebSocket: Bool = false
    private var finalResponseReceived: Bool = false
    private var connectionTimeoutWorkItem: DispatchWorkItem?
    private let transcriptLock = NSLock()
    private let audioBufferQueue = DispatchQueue(label: "com.dictum.audioBuffer")

    private var cachedConverter: AVAudioConverter?
    private var cachedInputFormat: AVAudioFormat?
    private var cachedOutputFormat: AVAudioFormat?

    override init() {
        super.init()
        urlSession = URLSession(configuration: .default, delegate: self, delegateQueue: nil)
    }

    deinit {
        urlSession?.invalidateAndCancel()
    }

    func startRecording(existingText: String = "") async {
        guard let apiKey = KeychainManager.shared.getAPIKey(), !apiKey.isEmpty else {
            await MainActor.run {
                errorMessage = "API –∫–ª—é—á –Ω–µ –Ω–∞–π–¥–µ–Ω. –û—Ç–∫—Ä–æ–π—Ç–µ –ù–∞—Å—Ç—Ä–æ–π–∫–∏"
            }
            return
        }

        let hasPermission = await requestMicrophonePermission()
        guard hasPermission else {
            await MainActor.run {
                errorMessage = "–ù–µ—Ç –¥–æ—Å—Ç—É–ø–∞ –∫ –º–∏–∫—Ä–æ—Ñ–æ–Ω—É"
            }
            return
        }

        let isAppend = !existingText.trimmingCharacters(in: .whitespacesAndNewlines).isEmpty

        transcriptLock.withLock { finalTranscript = "" }
        audioBufferQueue.sync { audioBuffer.removeAll() }
        webSocketConnected = false
        finalResponseReceived = false

        await MainActor.run {
            appendMode = isAppend
            interimText = ""
            transcriptionResult = nil
            isRecording = true
            audioLevel = 0.0
        }

        VolumeManager.shared.saveAndReduceVolume(targetVolume: SettingsManager.shared.volumeLevel)

        let language = SettingsManager.shared.preferredLanguage
        let model = SettingsManager.shared.deepgramModel
        guard let wsURL = URL(string: "wss://api.deepgram.com/v1/listen?encoding=linear16&sample_rate=16000&channels=1&model=\(model)&language=\(language)&interim_results=true&utterance_end_ms=2000&smart_format=true&punctuate=true") else {
            await MainActor.run { errorMessage = "–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è WebSocket URL" }
            return
        }

        var request = URLRequest(url: wsURL)
        request.setValue("Token \(apiKey)", forHTTPHeaderField: "Authorization")

        webSocket = urlSession.webSocketTask(with: request)
        webSocket?.resume()

        NSLog("üîå –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Deepgram WebSocket... model=\(model), language=\(language)")

        connectionTimeoutWorkItem?.cancel()
        connectionTimeoutWorkItem = DispatchWorkItem { [weak self] in
            guard let self = self else { return }
            if !self.webSocketConnected && self.isRecording {
                NSLog("‚ö†Ô∏è WebSocket timeout - –Ω–µ—Ç –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∑–∞ 5 —Å–µ–∫—É–Ω–¥")
                self.webSocket?.cancel(with: .goingAway, reason: nil)
                self.webSocket = nil
                DispatchQueue.main.async {
                    self.errorMessage = "Timeout –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ —Å–µ—Ä–≤–µ—Ä—É. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç."
                    self.isRecording = false
                }
                VolumeManager.shared.restoreVolume()
            }
        }
        if let workItem = connectionTimeoutWorkItem {
            DispatchQueue.main.asyncAfter(deadline: .now() + 5, execute: workItem)
        }

        receiveMessages()

        audioEngine = AVAudioEngine()
        guard let engine = audioEngine else {
            await MainActor.run { errorMessage = "–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∞—É–¥–∏–æ" }
            return
        }
        let inputNode = engine.inputNode
        let inputFormat = inputNode.outputFormat(forBus: 0)

        guard inputFormat.sampleRate > 0, inputFormat.channelCount > 0 else {
            await MainActor.run { errorMessage = "–ê—É–¥–∏–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ" }
            return
        }

        guard let outputFormat = AVAudioFormat(commonFormat: .pcmFormatInt16, sampleRate: 16000, channels: 1, interleaved: true) else {
            await MainActor.run { errorMessage = "–û—à–∏–±–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∞ –∞—É–¥–∏–æ" }
            return
        }

        audioEngine?.prepare()

        inputNode.installTap(onBus: 0, bufferSize: 1600, format: inputFormat) { [weak self] buffer, _ in
            self?.processAudioBuffer(buffer, from: inputFormat, to: outputFormat)
        }

        do {
            try audioEngine?.start()
            NSLog("üé§ –ê—É–¥–∏–æ –∑–∞–ø—É—â–µ–Ω (–±—É—Ñ–µ—Ä–∏–∑–∞—Ü–∏—è –¥–æ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è WebSocket)")
        } catch {
            await MainActor.run {
                errorMessage = "–û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞: \(error.localizedDescription)"
                isRecording = false
            }
            return
        }

        NSLog("üîå –û–∂–∏–¥–∞–Ω–∏–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è WebSocket...")
    }

    private func processAudioBuffer(_ buffer: AVAudioPCMBuffer, from inputFormat: AVAudioFormat, to outputFormat: AVAudioFormat) {
        // 1. –†–∞—Å—Å—á–∏—Ç–∞—Ç—å —É—Ä–æ–≤–µ–Ω—å –≥—Ä–æ–º–∫–æ—Å—Ç–∏
        if let channelData = buffer.floatChannelData {
            let frameLength = Int(buffer.frameLength)
            var sum: Float = 0.0
            for i in 0..<frameLength {
                let sample = channelData[0][i]
                sum += sample * sample
            }
            let rms = sqrt(sum / Float(max(1, frameLength)))
            let normalizedRms = rms * 50.0
            let level = min(1.0, log10(1 + normalizedRms * 9))

            DispatchQueue.main.async { [weak self] in
                self?.audioLevel = level
            }
        }

        // 2. –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –≤ 16kHz
        let converter: AVAudioConverter
        if let cached = cachedConverter,
           cachedInputFormat == inputFormat,
           cachedOutputFormat == outputFormat {
            converter = cached
        } else {
            guard let newConverter = AVAudioConverter(from: inputFormat, to: outputFormat) else { return }
            cachedConverter = newConverter
            cachedInputFormat = inputFormat
            cachedOutputFormat = outputFormat
            converter = newConverter
        }

        let ratio = outputFormat.sampleRate / inputFormat.sampleRate
        let outputFrameCount = AVAudioFrameCount(Double(buffer.frameLength) * ratio)

        guard let outputBuffer = AVAudioPCMBuffer(pcmFormat: outputFormat, frameCapacity: outputFrameCount) else { return }

        var error: NSError?
        let hasDataBox = BoolBox(true)

        converter.convert(to: outputBuffer, error: &error) { _, outStatus in
            if hasDataBox.value {
                outStatus.pointee = .haveData
                hasDataBox.value = false
                return buffer
            }
            outStatus.pointee = .noDataNow
            return nil
        }

        // 3. –û—Ç–ø—Ä–∞–≤–∏—Ç—å –∏–ª–∏ –±—É—Ñ–µ—Ä–∏–∑–∏—Ä–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ
        if error == nil, let channelData = outputBuffer.int16ChannelData {
            let byteCount = Int(outputBuffer.frameLength) * 2
            let data = Data(bytes: channelData[0], count: byteCount)

            audioBufferQueue.async { [weak self] in
                guard let self = self else { return }

                if self.webSocketConnected {
                    if !self.audioBuffer.isEmpty {
                        let count = self.audioBuffer.count
                        for bufferedData in self.audioBuffer {
                            self.webSocket?.send(.data(bufferedData)) { error in
                                if let error = error {
                                    NSLog("‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ –±—É—Ñ–µ—Ä–∏–∑–æ–≤–∞–Ω–Ω–æ–≥–æ –∞—É–¥–∏–æ: \(error.localizedDescription)")
                                }
                            }
                        }
                        self.audioBuffer.removeAll()
                        NSLog("üì§ –û—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ \(count) –±—É—Ñ–µ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤")
                    }
                    self.webSocket?.send(.data(data)) { error in
                        if let error = error {
                            NSLog("‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ –∞—É–¥–∏–æ: \(error.localizedDescription)")
                        }
                    }
                } else {
                    if self.audioBuffer.count < 30 {
                        self.audioBuffer.append(data)
                    } else {
                        NSLog("‚ö†Ô∏è –ë—É—Ñ–µ—Ä –∞—É–¥–∏–æ –ø–µ—Ä–µ–ø–æ–ª–Ω–µ–Ω! WebSocket –Ω–µ –ø–æ–¥–∫–ª—é—á–∞–µ—Ç—Å—è –±–æ–ª–µ–µ 3 —Å–µ–∫")
                    }
                }
            }
        }
    }

    func stopRecordingAndTranscribe(language: String) async {
        audioEngine?.inputNode.removeTap(onBus: 0)
        audioEngine?.stop()
        audioEngine = nil

        cachedConverter = nil
        cachedInputFormat = nil
        cachedOutputFormat = nil

        webSocket?.send(.string("{\"type\": \"CloseStream\"}")) { _ in }

        let deadline = Date().addingTimeInterval(2.0)
        while !finalResponseReceived && Date() < deadline {
            try? await Task.sleep(nanoseconds: 50_000_000)
        }

        if finalResponseReceived {
            NSLog("‚úÖ –ü–æ–ª—É—á–µ–Ω —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –æ—Ç Deepgram (speech_final –∏–ª–∏ UtteranceEnd)")
        } else {
            NSLog("‚ö†Ô∏è Timeout –æ–∂–∏–¥–∞–Ω–∏—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ (2 —Å–µ–∫)")
        }

        isClosingWebSocket = true
        webSocket?.cancel(with: .goingAway, reason: nil)
        webSocket = nil
        webSocketConnected = false
        DispatchQueue.main.asyncAfter(deadline: .now() + 0.5) { [weak self] in
            self?.isClosingWebSocket = false
        }

        let finalText = transcriptLock.withLock { finalTranscript }

        await MainActor.run {
            isRecording = false
            if !finalText.isEmpty {
                transcriptionResult = finalText.trimmingCharacters(in: .whitespaces)
            }
            interimText = ""
        }

        VolumeManager.shared.restoreVolume()

        NSLog("‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç: \(finalText)")
    }

    private func receiveMessages() {
        guard let webSocket = webSocket, !isClosingWebSocket else { return }

        webSocket.receive { [weak self] result in
            guard let self = self, self.webSocket != nil, !self.isClosingWebSocket else { return }

            switch result {
            case .success(let message):
                if case .string(let text) = message {
                    self.handleResponse(text)
                }
                if !self.isClosingWebSocket {
                    self.receiveMessages()
                }

            case .failure(let error):
                guard !self.isClosingWebSocket else { return }
                NSLog("‚ùå WS error: \(error.localizedDescription)")
                self.isClosingWebSocket = true
                self.webSocket?.cancel(with: .goingAway, reason: nil)
                self.webSocket = nil
                DispatchQueue.main.asyncAfter(deadline: .now() + 0.5) { [weak self] in
                    self?.isClosingWebSocket = false
                }
            }
        }
    }

    private func handleResponse(_ text: String) {
        NSLog("üì• Deepgram: \(text.prefix(500))...")

        guard let data = text.data(using: .utf8),
              let json = try? JSONSerialization.jsonObject(with: data) as? [String: Any] else {
            NSLog("‚ö†Ô∏è Deepgram: –Ω–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON")
            return
        }

        let messageType = json["type"] as? String ?? "unknown"

        if messageType == "Metadata" || messageType == "SpeechStarted" {
            NSLog("üìã Deepgram: —Å–ª—É–∂–µ–±–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ —Ç–∏–ø–∞ \(messageType)")
            return
        }

        if messageType == "UtteranceEnd" {
            if !finalResponseReceived {
                finalResponseReceived = true
                NSLog("üéØ UtteranceEnd received (fallback –¥–ª—è speech_final)")
            }
            return
        }

        guard let channel = json["channel"] as? [String: Any],
              let alternatives = channel["alternatives"] as? [[String: Any]],
              let transcript = alternatives.first?["transcript"] as? String else {
            NSLog("‚ö†Ô∏è Deepgram: –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞, type=\(messageType), keys=\(json.keys.joined(separator: ", "))")
            return
        }

        let isFinal = json["is_final"] as? Bool ?? false
        let speechFinal = json["speech_final"] as? Bool ?? false

        DispatchQueue.main.async {
            if isFinal && !transcript.isEmpty {
                self.transcriptLock.withLock {
                    self.finalTranscript += (self.finalTranscript.isEmpty ? "" : " ") + transcript
                }
                self.interimText = ""
                NSLog("üìù Final: \(transcript)")
            } else if !transcript.isEmpty {
                self.interimText = transcript
                NSLog("üìù Interim: \(transcript)")
            }

            if speechFinal {
                self.finalResponseReceived = true
                NSLog("üéØ Speech final received!")
            }
        }
    }

    private func requestMicrophonePermission() async -> Bool {
        await withCheckedContinuation { continuation in
            AVCaptureDevice.requestAccess(for: .audio) { granted in
                continuation.resume(returning: granted)
            }
        }
    }

    // URLSessionWebSocketDelegate
    func urlSession(_ session: URLSession, webSocketTask: URLSessionWebSocketTask, didOpenWithProtocol protocol: String?) {
        NSLog("‚úÖ WebSocket –ø–æ–¥–∫–ª—é—á–µ–Ω")

        webSocketConnected = true

        connectionTimeoutWorkItem?.cancel()
        connectionTimeoutWorkItem = nil

        audioBufferQueue.async { [weak self] in
            guard let self = self else { return }

            if !self.audioBuffer.isEmpty {
                let bufferedCount = self.audioBuffer.count
                for data in self.audioBuffer {
                    self.webSocket?.send(.data(data)) { error in
                        if let error = error {
                            NSLog("‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ –±—É—Ñ–µ—Ä–∏–∑–æ–≤–∞–Ω–Ω–æ–≥–æ –∞—É–¥–∏–æ: \(error.localizedDescription)")
                        }
                    }
                }
                self.audioBuffer.removeAll()
                NSLog("üì§ –û—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ \(bufferedCount) –±—É—Ñ–µ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ –∞—É–¥–∏–æ")
            }
        }
    }

    func urlSession(_ session: URLSession, webSocketTask: URLSessionWebSocketTask, didCloseWith closeCode: URLSessionWebSocketTask.CloseCode, reason: Data?) {
        NSLog("üîå WebSocket –∑–∞–∫—Ä—ã—Ç: \(closeCode.rawValue)")
        webSocketConnected = false
    }
}

// MARK: - ASR Provider Type
enum ASRProviderType: String, CaseIterable {
    case local = "local"
    case deepgram = "deepgram"

    var displayName: String {
        switch self {
        case .local: return "Parakeet v3 (–ª–æ–∫–∞–ª—å–Ω–∞—è)"
        case .deepgram: return "Deepgram (–æ–±–ª–∞–∫–æ)"
        }
    }

    var description: String {
        switch self {
        case .local: return "25 —è–∑—ã–∫–æ–≤, –æ—Ñ–ª–∞–π–Ω, ~190x real-time"
        case .deepgram: return "Streaming –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏"
        }
    }
}

// MARK: - Deepgram Error
enum DeepgramError: LocalizedError {
    case noAPIKey
    case invalidResponse
    case httpError(Int, String)
    case noTranscript

    var errorDescription: String? {
        switch self {
        case .noAPIKey:
            return "API –∫–ª—é—á –Ω–µ –Ω–∞–π–¥–µ–Ω. –û—Ç–∫—Ä–æ–π—Ç–µ –ù–∞—Å—Ç—Ä–æ–π–∫–∏ (Cmd+,) –∏ –≤–≤–µ–¥–∏—Ç–µ –∫–ª—é—á Deepgram."
        case .invalidResponse:
            return "–ù–µ–≤–µ—Ä–Ω—ã–π –æ—Ç–≤–µ—Ç –æ—Ç —Å–µ—Ä–≤–µ—Ä–∞"
        case .httpError(let code, let message):
            return "–û—à–∏–±–∫–∞ HTTP \(code): \(message)"
        case .noTranscript:
            return "–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç –Ω–µ –ø–æ–ª—É—á–µ–Ω"
        }
    }
}

// MARK: - Deepgram Response
struct DeepgramResponse: Codable {
    let metadata: Metadata?
    let results: Results

    struct Metadata: Codable {
        let request_id: String?
        let duration: Double?
    }

    struct Results: Codable {
        let channels: [Channel]
    }

    struct Channel: Codable {
        let alternatives: [Alternative]
    }

    struct Alternative: Codable {
        let transcript: String
        let confidence: Double
    }

    var transcript: String? {
        return results.channels.first?.alternatives.first?.transcript
    }
}

// MARK: - Deepgram Service (REST)
class DeepgramService {
    private let baseURL = "https://api.deepgram.com/v1/listen"

    func transcribe(audioURL: URL, language: String = "ru") async throws -> String {
        guard let apiKey = KeychainManager.shared.getAPIKey(), !apiKey.isEmpty else {
            throw DeepgramError.noAPIKey
        }

        let audioData = try Data(contentsOf: audioURL)
        NSLog("üì§ –û—Ç–ø—Ä–∞–≤–ª—è–µ–º: \(audioData.count) –±–∞–π—Ç, —è–∑—ã–∫: \(language)")

        if audioData.count < 1000 {
            throw DeepgramError.noTranscript
        }

        let model = SettingsManager.shared.deepgramModel
        guard var components = URLComponents(string: baseURL) else {
            throw DeepgramError.invalidResponse
        }
        components.queryItems = [
            URLQueryItem(name: "model", value: model),
            URLQueryItem(name: "language", value: language),
            URLQueryItem(name: "smart_format", value: "true"),
            URLQueryItem(name: "punctuate", value: "true")
        ]

        guard let url = components.url else {
            throw DeepgramError.invalidResponse
        }
        var request = URLRequest(url: url)
        request.httpMethod = "POST"
        request.setValue("Token \(apiKey)", forHTTPHeaderField: "Authorization")
        request.setValue("audio/wav", forHTTPHeaderField: "Content-Type")
        request.httpBody = audioData
        request.timeoutInterval = 30

        NSLog("üì° –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤ Deepgram...")
        let startTime = Date()

        let (data, response) = try await URLSession.shared.data(for: request)

        let elapsed = Date().timeIntervalSince(startTime)
        NSLog("‚è±Ô∏è –û—Ç–≤–µ—Ç –∑–∞ \(String(format: "%.2f", elapsed)) —Å–µ–∫")

        guard let httpResponse = response as? HTTPURLResponse else {
            throw DeepgramError.invalidResponse
        }

        if httpResponse.statusCode != 200 {
            let errorMsg = String(data: data, encoding: .utf8) ?? "Unknown"
            NSLog("‚ùå HTTP \(httpResponse.statusCode): \(errorMsg)")
            throw DeepgramError.httpError(httpResponse.statusCode, errorMsg)
        }

        let deepgramResponse = try JSONDecoder().decode(DeepgramResponse.self, from: data)

        guard let transcript = deepgramResponse.transcript, !transcript.isEmpty else {
            NSLog("‚ö†Ô∏è –ü—É—Å—Ç–æ–π —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç")
            throw DeepgramError.noTranscript
        }

        NSLog("‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç: \(transcript)")
        return transcript
    }
}

// MARK: - SwiftUI Previews
#Preview("ParakeetModelStatus") {
    VStack(alignment: .leading, spacing: 8) {
        Text(ParakeetModelStatus.notChecked.displayText)
        Text(ParakeetModelStatus.notDownloaded.displayText)
        Text(ParakeetModelStatus.downloading.displayText)
        Text(ParakeetModelStatus.loading.displayText)
        Text(ParakeetModelStatus.ready.displayText)
        Text(ParakeetModelStatus.error("Test error").displayText)
    }
    .padding()
    .background(Color.black)
    .foregroundColor(.white)
}

#Preview("ASRProviderType") {
    VStack(alignment: .leading, spacing: 8) {
        ForEach(ASRProviderType.allCases, id: \.rawValue) { provider in
            VStack(alignment: .leading) {
                Text(provider.displayName)
                    .font(.headline)
                Text(provider.description)
                    .font(.caption)
                    .foregroundColor(.gray)
            }
        }
    }
    .padding()
    .background(Color.black)
    .foregroundColor(.white)
}
