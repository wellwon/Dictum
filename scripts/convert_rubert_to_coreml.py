#!/usr/bin/env python3
"""
–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è rubert-tiny2 –≤ CoreML –¥–ª—è Dictum TextSwitcher

–ú–æ–¥–µ–ª—å: cointegrated/rubert-tiny2 (DeepPavlov)
- –†–∞–∑–º–µ—Ä: ~30MB
- –°–∫–æ—Ä–æ—Å—Ç—å: <2ms –Ω–∞ Apple Neural Engine
- –ó–∞–¥–∞—á–∞: Masked Language Modeling –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    pip install torch transformers coremltools numpy
    python convert_rubert_to_coreml.py

–†–µ–∑—É–ª—å—Ç–∞—Ç:
    - RuBertTiny2.mlpackage (~30MB)
    - vocab.txt (—Å–ª–æ–≤–∞—Ä—å —Ç–æ–∫–µ–Ω–æ–≤)
"""

import os
import sys
import shutil
from pathlib import Path

import numpy as np
import torch
import coremltools as ct
from transformers import AutoModelForMaskedLM, AutoTokenizer

# === –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è ===
MODEL_NAME = "cointegrated/rubert-tiny2"
OUTPUT_DIR = Path(__file__).parent.parent / "Dictum" / "Resources"
MLPACKAGE_NAME = "RuBertTiny2.mlpackage"
VOCAB_NAME = "vocab.txt"
MAX_SEQ_LENGTH = 64  # –ö–æ—Ä–æ—Ç–∫–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ 64)


def download_model():
    """–°–∫–∞—á–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å HuggingFace."""
    print(f"üì• –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ {MODEL_NAME}...")

    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    model = AutoModelForMaskedLM.from_pretrained(MODEL_NAME, torchscript=False)
    model.eval()

    print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {model.config.hidden_size} hidden, {model.config.num_hidden_layers} layers")
    return model, tokenizer


def trace_model(model, tokenizer):
    """–¢—Ä–µ–π—Å–∏—Ç –º–æ–¥–µ–ª—å –¥–ª—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ –≤ CoreML."""
    print("üîß –¢—Ä–µ–π—Å–∏–Ω–≥ –º–æ–¥–µ–ª–∏...")

    # –ü—Ä–∏–º–µ—Ä –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    sample_text = "–ü—Ä–∏–≤–µ—Ç –∫–∞–∫ [MASK] –¥–µ–ª–∞"
    inputs = tokenizer(
        sample_text,
        return_tensors="pt",
        padding="max_length",
        truncation=True,
        max_length=MAX_SEQ_LENGTH
    )

    input_ids = inputs["input_ids"]
    attention_mask = inputs["attention_mask"]

    # –û–±–æ—Ä–∞—á–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ç–æ–ª—å–∫–æ logits
    class BertWrapper(torch.nn.Module):
        def __init__(self, bert_model):
            super().__init__()
            self.bert = bert_model

        def forward(self, input_ids, attention_mask):
            outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
            return outputs.logits  # [batch, seq_len, vocab_size]

    wrapped_model = BertWrapper(model)
    wrapped_model.eval()

    # –¢—Ä–µ–π—Å–∏–Ω–≥
    with torch.no_grad():
        traced_model = torch.jit.trace(wrapped_model, (input_ids, attention_mask))

    print("‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ —Ç—Ä–µ–π—Å–Ω—É—Ç–∞")
    return traced_model, tokenizer


def convert_to_coreml(traced_model, tokenizer):
    """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç —Ç—Ä–µ–π—Å–Ω—É—Ç—É—é –º–æ–¥–µ–ª—å –≤ CoreML."""
    print("üîÑ –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ CoreML...")

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≤—Ö–æ–¥—ã
    input_ids_shape = ct.Shape(shape=(1, MAX_SEQ_LENGTH))
    attention_mask_shape = ct.Shape(shape=(1, MAX_SEQ_LENGTH))

    # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è
    mlmodel = ct.convert(
        traced_model,
        inputs=[
            ct.TensorType(name="input_ids", shape=input_ids_shape, dtype=np.int32),
            ct.TensorType(name="attention_mask", shape=attention_mask_shape, dtype=np.int32),
        ],
        outputs=[
            ct.TensorType(name="logits", dtype=np.float32),
        ],
        convert_to="mlprogram",  # –°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç (.mlpackage)
        compute_units=ct.ComputeUnit.ALL,  # CPU + GPU + Neural Engine
        minimum_deployment_target=ct.target.macOS14,  # macOS Sonoma
    )

    # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    mlmodel.author = "Dictum"
    mlmodel.short_description = "rubert-tiny2 –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Ä–∞—Å–∫–ª–∞–¥–∫–∏"
    mlmodel.version = "1.0"

    # –í—Ö–æ–¥—ã/–≤—ã—Ö–æ–¥—ã
    mlmodel.input_description["input_ids"] = "–¢–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç (WordPiece)"
    mlmodel.input_description["attention_mask"] = "–ú–∞—Å–∫–∞ –≤–Ω–∏–º–∞–Ω–∏—è (1=—Ç–æ–∫–µ–Ω, 0=–ø–∞–¥–¥–∏–Ω–≥)"
    mlmodel.output_description["logits"] = "–õ–æ–≥–∏—Ç—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ [1, seq_len, vocab_size]"

    print(f"‚úÖ –ú–æ–¥–µ–ª—å –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–∞ –≤ CoreML (compute_units=ALL)")
    return mlmodel


def save_vocab(tokenizer, output_dir):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–ª–æ–≤–∞—Ä—å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞."""
    vocab_path = output_dir / VOCAB_NAME

    # –ü–æ–ª—É—á–∞–µ–º vocab
    vocab = tokenizer.get_vocab()

    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∏–Ω–¥–µ–∫—Å—É
    sorted_vocab = sorted(vocab.items(), key=lambda x: x[1])

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º
    with open(vocab_path, "w", encoding="utf-8") as f:
        for token, idx in sorted_vocab:
            f.write(f"{token}\n")

    print(f"‚úÖ –°–ª–æ–≤–∞—Ä—å —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {vocab_path} ({len(vocab)} —Ç–æ–∫–µ–Ω–æ–≤)")
    return vocab_path


def save_model(mlmodel, output_dir):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç CoreML –º–æ–¥–µ–ª—å."""
    output_path = output_dir / MLPACKAGE_NAME

    # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—É—é –≤–µ—Ä—Å–∏—é –µ—Å–ª–∏ –µ—Å—Ç—å
    if output_path.exists():
        shutil.rmtree(output_path)

    mlmodel.save(str(output_path))

    # –†–∞–∑–º–µ—Ä
    size_mb = sum(f.stat().st_size for f in output_path.rglob("*") if f.is_file()) / (1024 * 1024)
    print(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {output_path} ({size_mb:.1f} MB)")

    return output_path


def verify_model(mlmodel, tokenizer):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ä–∞–±–æ—Ç—É –º–æ–¥–µ–ª–∏."""
    print("üß™ –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–æ–¥–µ–ª–∏...")

    # –¢–µ—Å—Ç–æ–≤—ã–π —Ç–µ–∫—Å—Ç
    test_text = "–ü—Ä–∏–≤–µ—Ç –∫–∞–∫ [MASK] –¥–µ–ª–∞"
    inputs = tokenizer(
        test_text,
        return_tensors="np",
        padding="max_length",
        truncation=True,
        max_length=MAX_SEQ_LENGTH
    )

    # –ò–Ω—Ñ–µ—Ä–µ–Ω—Å
    prediction = mlmodel.predict({
        "input_ids": inputs["input_ids"].astype(np.int32),
        "attention_mask": inputs["attention_mask"].astype(np.int32),
    })

    logits = prediction["logits"]

    # –ù–∞—Ö–æ–¥–∏–º –ø–æ–∑–∏—Ü–∏—é [MASK]
    mask_token_id = tokenizer.mask_token_id
    mask_pos = np.where(inputs["input_ids"][0] == mask_token_id)[0]

    if len(mask_pos) > 0:
        mask_logits = logits[0, mask_pos[0], :]
        top_5_ids = np.argsort(mask_logits)[-5:][::-1]
        top_5_tokens = [tokenizer.decode([idx]) for idx in top_5_ids]
        print(f"   –¢–µ—Å—Ç: '{test_text}'")
        print(f"   Top-5 –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –¥–ª—è [MASK]: {top_5_tokens}")

    print("‚úÖ –ú–æ–¥–µ–ª—å —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏."""
    print("=" * 60)
    print("üöÄ –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è rubert-tiny2 ‚Üí CoreML –¥–ª—è Dictum")
    print("=" * 60)

    # –°–æ–∑–¥–∞—ë–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

    # –®–∞–≥ 1: –°–∫–∞—á–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å
    model, tokenizer = download_model()

    # –®–∞–≥ 2: –¢—Ä–µ–π—Å–∏–º –º–æ–¥–µ–ª—å
    traced_model, tokenizer = trace_model(model, tokenizer)

    # –®–∞–≥ 3: –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ CoreML
    mlmodel = convert_to_coreml(traced_model, tokenizer)

    # –®–∞–≥ 4: –°–æ—Ö—Ä–∞–Ω—è–µ–º vocab
    save_vocab(tokenizer, OUTPUT_DIR)

    # –®–∞–≥ 5: –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
    save_model(mlmodel, OUTPUT_DIR)

    # –®–∞–≥ 6: –ü—Ä–æ–≤–µ—Ä—è–µ–º
    verify_model(mlmodel, tokenizer)

    print("=" * 60)
    print("üéâ –ì–æ—Ç–æ–≤–æ! –§–∞–π–ª—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤:")
    print(f"   üì¶ {OUTPUT_DIR / MLPACKAGE_NAME}")
    print(f"   üìÑ {OUTPUT_DIR / VOCAB_NAME}")
    print("=" * 60)
    print("\n–°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:")
    print("1. –î–æ–±–∞–≤–∏—Ç—å RuBertTiny2.mlpackage –≤ Xcode –ø—Ä–æ–µ–∫—Ç")
    print("2. –î–æ–±–∞–≤–∏—Ç—å vocab.txt –≤ Resources")
    print("3. –ù–∞–ø–∏—Å–∞—Ç—å BertTokenizer.swift")
    print("4. –ù–∞–ø–∏—Å–∞—Ç—å NeuralContextBrain.swift")


if __name__ == "__main__":
    main()
